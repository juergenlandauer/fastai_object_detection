{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics\n",
    "\n",
    "> Definition of the metrics that can be used to evaluate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from nbdev.showdoc import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastai_object_detection.external.mean_average_precision_source import MetricBuilder\n",
    "from fastai.metrics import Metric\n",
    "from fastai.torch_basics import *\n",
    "from fastai.torch_core import *\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export     \n",
    "class mAP_Metric():\n",
    "    \"Metric to calculate mAP for different IoU thresholds\"\n",
    "    def __init__(self, iou_thresholds, recall_thresholds=None, mpolicy=\"greedy\", name=\"mAP\", remove_background_class=True):\n",
    "        self.__name__ = name\n",
    "        self.iou_thresholds = iou_thresholds\n",
    "        self.recall_thresholds = recall_thresholds\n",
    "        self.mpolicy = mpolicy\n",
    "        self.remove_background_class = remove_background_class\n",
    "        \n",
    "    def __call__(self, preds, targs, num_classes):\n",
    "        if self.remove_background_class:\n",
    "            num_classes=num_classes-1\n",
    "        #metric_fn = MetricBuilder.build_evaluation_metric(\"map_2d\", async_mode=True, num_classes=num_classes)\n",
    "        metric_fn = MetricBuilder.build_evaluation_metric(\"map_2d\", async_mode=False, num_classes=num_classes)\n",
    "        for sample_preds, sample_targs in self.create_metric_samples(preds, targs):\n",
    "            metric_fn.add(sample_preds, sample_targs)\n",
    "        metric_batch = metric_fn.value(iou_thresholds=self.iou_thresholds,\n",
    "                                       recall_thresholds=self.recall_thresholds, \n",
    "                                       mpolicy=self.mpolicy)['mAP']\n",
    "        return metric_batch\n",
    "    \n",
    "    def create_metric_samples(self, preds, targs):\n",
    "        pred_samples = []\n",
    "        for pred in preds:\n",
    "            res = torch.cat([pred[\"boxes\"], pred[\"labels\"].unsqueeze(-1), pred[\"scores\"].unsqueeze(-1)], dim=1) \n",
    "            pred_np = res.detach().cpu()#.numpy()\n",
    "            if self.remove_background_class:\n",
    "                # first idx is background\n",
    "                try:\n",
    "                    pred_np= pred_np-np.array([0,0,0,0,1,0])\n",
    "                except: pass\n",
    "            pred_samples.append(pred_np)\n",
    "\n",
    "        targ_samples = []\n",
    "        for targ in targs: # targs : yb[0]\n",
    "            targ = torch.cat([targ[\"boxes\"],targ[\"labels\"].unsqueeze(-1)], dim=1)\n",
    "            targ = torch.cat([targ, torch.zeros([targ.shape[0], 2], device=targ.device)], dim=1)\n",
    "            targ_np = targ.detach().cpu()\n",
    "            #targ_np = np.array(targ.detach().cpu())\n",
    "            if self.remove_background_class:\n",
    "                # first idx is background \n",
    "                try:\n",
    "                    targ_np= targ_np-np.array([0,0,0,0,1,0,0])\n",
    "                except: pass\n",
    "            targ_samples.append(targ_np)\n",
    "\n",
    "        return [s for s in zip(pred_samples, targ_samples)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "\n",
    "class _AvgMetric_ObjectDetection(Metric):\n",
    "    \"Average the values of `func` taking into account potential different batch sizes\"\n",
    "    def __init__(self, func): self.func = func\n",
    "    def reset(self): self.total,self.count = 0.,0\n",
    "    def accumulate(self, learn):\n",
    "        bs = len(learn.xb[0])\n",
    "        self.total += learn.to_detach(self.func(learn.pred, *learn.yb, num_classes=len(learn.dls.vocab)))*bs\n",
    "        self.count += bs\n",
    "    @property\n",
    "    def value(self): return self.total/self.count if self.count != 0 else None\n",
    "    @property\n",
    "    def name(self): return self.func.func.__name__ if hasattr(self.func, 'func') else  self.func.__name__\n",
    "    \n",
    "           \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to create mAP metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "\n",
    "def create_mAP_metric(iou_tresh, recall_thresholds, mpolicy, metric_name, remove_background_class=True):\n",
    "    \"\"\" Creates a function to pass into learner for measuring mAP.\n",
    "    iou_tresh: float or np.arange, f.e.: np.arange(0.5, 1.0, 0.05)\n",
    "    recall_thresholds: None or np.arange, f.e.: np.arange(0., 1.01, 0.01)\n",
    "    mpolicy: str, 'soft' or 'greedy'\n",
    "    metric_name: str, name to display in fastaiÂ´s recorder\n",
    "    remove_background_class: True or False, remove first index before evaluation, as it represents background class in our dataloader\n",
    "    Metric Examples:\n",
    "    COCO mAP: set recall_thresholds=np.arange(0., 1.01, 0.01), mpolicy=\"soft\"\n",
    "    VOC PASCAL mAP: set recall_thresholds=np.arange(0., 1.1, 0.1), mpolicy=\"greedy\"\n",
    "    VOC PASCAL mAP in all points: set recall_thresholds=None, mpolicy=\"greedy\"\n",
    "    \"\"\"\n",
    "    return _AvgMetric_ObjectDetection(mAP_Metric(iou_tresh, recall_thresholds=recall_thresholds, mpolicy=mpolicy,\n",
    "                                                    name=metric_name, remove_background_class=True)) \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "# coco mAP    \n",
    "mAP_at_IoU40 = _AvgMetric_ObjectDetection(mAP_Metric(0.4, recall_thresholds=np.arange(0., 1.01, 0.01), mpolicy=\"soft\",\n",
    "                                                    name=\"mAP@IoU>0.4\", remove_background_class=True))\n",
    "mAP_at_IoU50 = _AvgMetric_ObjectDetection(mAP_Metric(0.5, recall_thresholds=np.arange(0., 1.01, 0.01), mpolicy=\"soft\",\n",
    "                                                    name=\"mAP@IoU>0.5\", remove_background_class=True))\n",
    "mAP_at_IoU60 = _AvgMetric_ObjectDetection(mAP_Metric(0.6, recall_thresholds=np.arange(0., 1.01, 0.01), mpolicy=\"soft\",\n",
    "                                                    name=\"mAP@IoU>0.6\", remove_background_class=True))\n",
    "mAP_at_IoU70 = _AvgMetric_ObjectDetection(mAP_Metric(0.7, recall_thresholds=np.arange(0., 1.01, 0.01), mpolicy=\"soft\",\n",
    "                                                    name=\"mAP@IoU>0.7\", remove_background_class=True))\n",
    "mAP_at_IoU80 = _AvgMetric_ObjectDetection(mAP_Metric(0.8, recall_thresholds=np.arange(0., 1.01, 0.01), mpolicy=\"soft\",\n",
    "                                                    name=\"mAP@IoU>0.8\", remove_background_class=True))\n",
    "mAP_at_IoU90 = _AvgMetric_ObjectDetection(mAP_Metric(0.9, recall_thresholds=np.arange(0., 1.01, 0.01), mpolicy=\"soft\",\n",
    "                                                    name=\"mAP@IoU>0.9\", remove_background_class=True))\n",
    "mAP_at_IoU50_95 = _AvgMetric_ObjectDetection(mAP_Metric(np.arange(0.5, 1.0, 0.05), recall_thresholds=np.arange(0., 1.01, 0.01), mpolicy=\"soft\",\n",
    "                                                    name=\"mAP@IoU 0.5:0.95\", remove_background_class=True)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom mAP metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we create some predictions and targets. Note that our dataloader contains a background class with index 0 and all metrics remove by default the background class, so the first class has index 1 and the number of classes is 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'boxes': tensor([[439, 157, 556, 241],\n",
       "          [437, 246, 518, 351],\n",
       "          [515, 306, 595, 375],\n",
       "          [407, 386, 531, 476],\n",
       "          [544, 419, 621, 476],\n",
       "          [609, 297, 636, 392]]),\n",
       "  'labels': tensor([1, 1, 1, 1, 1, 1])}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boxes = torch.tensor([\n",
    "    [439, 157, 556, 241],\n",
    "    [437, 246, 518, 351],\n",
    "    [515, 306, 595, 375],\n",
    "    [407, 386, 531, 476],\n",
    "    [544, 419, 621, 476],\n",
    "    [609, 297, 636, 392]])\n",
    "labels = torch.ones(6, dtype=torch.long)\n",
    "\n",
    "targs = [dict({\"boxes\":boxes, \"labels\":labels})]\n",
    "targs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'boxes': tensor([[429, 219, 528, 247],\n",
       "          [433, 260, 506, 336],\n",
       "          [518, 314, 603, 369],\n",
       "          [592, 310, 634, 388],\n",
       "          [403, 384, 517, 461],\n",
       "          [405, 429, 519, 470],\n",
       "          [433, 272, 499, 341],\n",
       "          [413, 390, 515, 459]]),\n",
       "  'labels': tensor([1, 1, 1, 1, 1, 1, 1, 1]),\n",
       "  'scores': tensor([0.4609, 0.2698, 0.4626, 0.2982, 0.3829, 0.3694, 0.2728, 0.6195])}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boxes = torch.tensor([\n",
    "    [429, 219, 528, 247],\n",
    "    [433, 260, 506, 336],\n",
    "    [518, 314, 603, 369],\n",
    "    [592, 310, 634, 388],\n",
    "    [403, 384, 517, 461],\n",
    "    [405, 429, 519, 470],\n",
    "    [433, 272, 499, 341],\n",
    "    [413, 390, 515, 459]])\n",
    "labels = torch.ones(8, dtype=torch.long)\n",
    "scores = torch.tensor([0.460851, 0.269833, 0.462608, 0.298196, 0.382881, 0.369369, 0.272826, 0.619459])\n",
    "\n",
    "preds = [dict({\"boxes\":boxes, \"labels\":labels, \"scores\":scores})]\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VOC PASCAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5000)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc_pascal = create_mAP_metric(0.5, np.arange(0., 1.1, 0.1), \"greedy\", \"VOC PASCAL mAP\", \n",
    "                               remove_background_class=True)\n",
    "voc_pascal.func(preds, targs, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5000)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc_pascal_all_pnts = create_mAP_metric(0.5, None, \"greedy\", \"VOC PASCAL mAP all points\", \n",
    "                                        remove_background_class=True)\n",
    "voc_pascal_all_pnts.func(preds, targs, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COCO mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5000)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coco_map_50 = create_mAP_metric(0.5, np.arange(0., 1.01, 0.01), \"soft\", \"COCO mAP@0.5\", \n",
    "                                remove_background_class=True)\n",
    "coco_map_50.func(preds, targs, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1573)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coco_map_50_95 = create_mAP_metric(np.arange(0.5, 1, .05), np.arange(0., 1.01, 0.01), \"soft\", \"COCO mAP@[0.5:0.95]\", \n",
    "                                remove_background_class=True)\n",
    "coco_map_50_95.func(preds, targs, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_close(voc_pascal.func(preds, targs, num_classes=2), 0.5, eps=1e-03)\n",
    "test_close(voc_pascal_all_pnts.func(preds, targs, num_classes=2), 0.5, eps=1e-03)\n",
    "test_close(coco_map_50.func(preds, targs, num_classes=2), 0.5, eps=1e-03)\n",
    "test_close(coco_map_50_95.func(preds, targs, num_classes=2), 0.157, eps=1e-03)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prebuilt metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some prebuilt metrics, which you can use instantly:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COCO mAP:\n",
    "* `mAP_at_IoU40` \n",
    "* `mAP_at_IoU50` \n",
    "* `mAP_at_IoU60`\n",
    "* `mAP_at_IoU70` \n",
    "* `mAP_at_IoU80`\n",
    "* `mAP_at_IoU90` \n",
    "* `mAP_at_IoU50_95` (mAP@[0.50:0.95:0.05])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
